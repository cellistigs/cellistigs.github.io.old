
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Are More Training Samples Always Good? &#8212; taiga_projectdoc  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="author" title="About these documents" href="../../about/" />
    <link rel="index" title="Index" href="../../genindex/" />
    <link rel="search" title="Search" href="../../search/" /> 
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />
 
<link
  rel="alternate"
  type="application/atom+xml"
  href="../../blog/atom.xml"
  title="taiga_projectdoc Blog"
/>
 
<style type="text/css">
  ul.ablog-archive {
    list-style: none;
    overflow: auto;
    margin-left: 0px;
  }
  ul.ablog-archive li {
    float: left;
    margin-right: 5px;
    font-size: 80%;
  }
  ul.postlist a {
    font-style: italic;
  }
  ul.postlist-style-disc {
    list-style-type: disc;
  }
  ul.postlist-style-none {
    list-style-type: none;
  }
  ul.postlist-style-circle {
    list-style-type: circle;
  }
</style>

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
             <section id="are-more-training-samples-always-good">
<h1>Are More Training Samples Always Good?<a class="headerlink" href="#are-more-training-samples-always-good" title="Permalink to this headline">¶</a></h1>
<p>Common knowledge in the machine learning community is that adding more data is always a good idea, and low data methods like n-shot learning (for small n) normally require the introduction of strong inductive biases in order to achieve good performance. However, if we think about the eventual goal of our networks as generalization, does adding more training data really always help? Some recent studies would suggest that the answer is no.</p>
<section id="pleiss-2020">
<h2>Pleiss 2020<a class="headerlink" href="#pleiss-2020" title="Permalink to this headline">¶</a></h2>
<p>First, consider the case of <span id="id1">[<a class="reference internal" href="#id21" title="Geoff Pleiss, Tianyi Zhang, Ethan R Elenberg, and Kilian Q Weinberger. Identifying Mislabeled Data using the Area Under the Margin Ranking. arXiv, 2020. arXiv:2001.10528.">PZEW20</a>]</span>. This paper introduces a new metric, AUM (area under margin) that keeps track of the relative probability afforded to a given sample’s classification output over the course of training. AUM produces a ranking at the end of some training period, with highest ranked samples representing those that are somehow most representative of a given class, and lowest ranked samples representing those that are least representative. Surprisingly, the authors show that by removing low AUM samples based on a given threshold, network performance can be dramatically increased, even outperforming the same networks with data augmentation. While this could be an effect of mislabeled data, there is no way for AUM to distinguish between mislabeled and simply challenging samples, making this an interesting result for further exploration.</p>
</section>
<section id="feldman-2020">
<h2>Feldman 2020<a class="headerlink" href="#feldman-2020" title="Permalink to this headline">¶</a></h2>
<p>Next, consider <span id="id2">[<a class="reference internal" href="#id41" title="Vitaly Feldman and Chiyuan Zhang. What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation. arXiv, 2020. This is a super cool paper to deal into heterogeneity in a training dataset, understood as the idea of long tails. Long tails in the dataset correspond to distinct groups of examples that might be very uncommon, but nevertheless reflect the true data distribution as might be represented in a test set. The paper gives a (more) efficient way to estimate per-datapoint values that can identify these distinct groups, and also quantify their usefulness in the test set. It still seems like the methods here are incredibly computationally intensive, but some nice ideas that might come out of this: - all data points in the training set are not equal. Some are essentially worthless, while others are uniquely valuable. This might lend itself to the idea of heterogeneous, hierarchical data representation, where certain examples capture a disproportionately large fraction of the model’s representative power, but that something about your data distribution justifies that. - a neural networks learned deep representation formation naturally performs some kind of data compression, triaging, something, so that the whole data distribution can be well represented. - it’s unclear how these ideas generalize to video data, or the kind of datasets that neuroscientists care about.">FZ20</a>]</span>. This paper does not look at training dynamics, but rather the performance of a family of networks that subsample the full training set. For each training frame, we then ask what the marginal effect of including or excluding that training frame was on performance, somewhat similar to the <a class="reference external" href="https://christophm.github.io/interpretable-ml-book/shapley.html">Shapley value</a>  as it has been used in Machine Learning previously. The paper introduces new efficient estimators to <em>influence functions</em> that measure the amount to which a particular training datapoint changes the probability of any test data’s groundtruth category being correctly selected. Although their interest is in measuring the distribution of positive influence functions, if we look into their <cite>precomputed influence functions &lt;https://pluskid.github.io/influence-memorization/&gt;</cite>, we see negative influence values, at the very least in CIFAR data, even when only looking at influence between points with the same class assignment. While increasing model uncertainty is definitely a good idea (and in general, most models are overconfident), its unclear if the influence of each data point is strictly positive.</p>
</section>
<section id="nakkiran-2019">
<h2>Nakkiran 2019<a class="headerlink" href="#nakkiran-2019" title="Permalink to this headline">¶</a></h2>
<p>Finally, in a slightly different setting, <span id="id3">[<a class="reference internal" href="#id16" title="Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep Double Descent: Where Bigger Models and More Data Hurt. arXiv, 2019. Gives empirical evidence for the idea that you first learn a function with small generalization gap, then you memorize samples, and then afterwards if your model is large enough you can find a model configuration where you can accommodate the memorized samples and still carry through the right inference on test data. arXiv:1912.02292.">NKB+19</a>]</span> show the existence of “double descent curves” that characterize the change in performance of a family of network models as you increase 1) the number of parameters and 2) the number of training epochs. They show that there is a training-set dependent peak in the cost as you increase both of these parameters in Figure 11b and 15, both for deep networks and for linear models (Random Fourier Features). They hypothesize that this is because models may become more or less sensitive to even small amounts of noise in the training set.</p>
</section>
<section id="ngiam-2018">
<h2>Ngiam 2018<a class="headerlink" href="#ngiam-2018" title="Permalink to this headline">¶</a></h2>
<p>This paper looks at pretraining: TO READ</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>All of these examples assume or show that there is some noise in the training labels, whether that noise be misassignments, or simple ambiguity. While some might argue that there is no label noise in large datasets (see citations of <span id="id4">[<a class="reference internal" href="#id41" title="Vitaly Feldman and Chiyuan Zhang. What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation. arXiv, 2020. This is a super cool paper to deal into heterogeneity in a training dataset, understood as the idea of long tails. Long tails in the dataset correspond to distinct groups of examples that might be very uncommon, but nevertheless reflect the true data distribution as might be represented in a test set. The paper gives a (more) efficient way to estimate per-datapoint values that can identify these distinct groups, and also quantify their usefulness in the test set. It still seems like the methods here are incredibly computationally intensive, but some nice ideas that might come out of this: - all data points in the training set are not equal. Some are essentially worthless, while others are uniquely valuable. This might lend itself to the idea of heterogeneous, hierarchical data representation, where certain examples capture a disproportionately large fraction of the model’s representative power, but that something about your data distribution justifies that. - a neural networks learned deep representation formation naturally performs some kind of data compression, triaging, something, so that the whole data distribution can be well represented. - it’s unclear how these ideas generalize to video data, or the kind of datasets that neuroscientists care about.">FZ20</a>, <a class="reference internal" href="#id21" title="Geoff Pleiss, Tianyi Zhang, Ethan R Elenberg, and Kilian Q Weinberger. Identifying Mislabeled Data using the Area Under the Margin Ranking. arXiv, 2020. arXiv:2001.10528.">PZEW20</a>]</span>) it appears that we’ll always have these kinds of ambiguities in our training set. For the case of keypoint detection, this is more true than in many other cases, as target maps might overlap with bad features and create distractors. Understanding the level of ambiguity introduced by a certain training point, and incorporating that ambiguity into the training cost, as considered by other training objectives (<span id="id5">[<a class="reference internal" href="#id40" title="missing journal in Chapelle.2001">CWBV01</a>, <a class="reference internal" href="#id26" title="Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond Empirical Risk Minimization. arXiv, 2017. arXiv:1710.09412.">ZCDLP17</a>]</span>)</p>
</section>
<section id="bibliography">
<h2>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h2>
<p id="id6"><dl class="citation">
<dt class="label" id="id40"><span class="brackets"><a class="fn-backref" href="#id5">CWBV01</a></span></dt>
<dd><p><strong>missing journal in Chapelle.2001</strong></p>
</dd>
<dt class="label" id="id41"><span class="brackets">FZ20</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>Vitaly Feldman and Chiyuan Zhang. What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation. <em>arXiv</em>, 2020. This is a super cool paper to deal into heterogeneity in a training dataset, understood as the idea of long tails. Long tails in the dataset correspond to distinct groups of examples that might be very uncommon, but nevertheless reflect the true data distribution as might be represented in a test set. The paper gives a (more) efficient way to estimate per-datapoint values that can identify these distinct groups, and also quantify their usefulness in the test set. It still seems like the methods here are incredibly computationally intensive, but some nice ideas that might come out of this: - all data points in the training set are not equal. Some are essentially worthless, while others are uniquely valuable. This might lend itself to the idea of heterogeneous, hierarchical data representation, where certain examples capture a disproportionately large fraction of the model’s representative power, but that something about your data distribution justifies that. - a neural networks learned deep representation formation naturally performs some kind of data compression, triaging, something, so that the whole data distribution can be well represented. - it’s unclear how these ideas generalize to video data, or the kind of datasets that neuroscientists care about.</p>
</dd>
<dt class="label" id="id16"><span class="brackets"><a class="fn-backref" href="#id3">NKB+19</a></span></dt>
<dd><p>Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep Double Descent: Where Bigger Models and More Data Hurt. <em>arXiv</em>, 2019. Gives empirical evidence for the idea that you first learn a function with small generalization gap, then you memorize samples, and then afterwards if your model is large enough you can find a model configuration where you can accommodate the memorized samples and still carry through the right inference on test data. <a class="reference external" href="https://arxiv.org/abs/1912.02292">arXiv:1912.02292</a>.</p>
</dd>
<dt class="label" id="id21"><span class="brackets">PZEW20</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>Geoff Pleiss, Tianyi Zhang, Ethan R Elenberg, and Kilian Q Weinberger. Identifying Mislabeled Data using the Area Under the Margin Ranking. <em>arXiv</em>, 2020. <a class="reference external" href="https://arxiv.org/abs/2001.10528">arXiv:2001.10528</a>.</p>
</dd>
<dt class="label" id="id34"><span class="brackets">RMS+20</span></dt>
<dd><p>Colorado J Reed, Sean Metzger, Aravind Srinivas, Trevor Darrell, and Kurt Keutzer. SelfAugment: Automatic Augmentation Policies for Self-Supervised Learning. <em>arXiv</em>, 2020. <a class="reference external" href="https://arxiv.org/abs/2009.07724">arXiv:2009.07724</a>.</p>
</dd>
<dt class="label" id="id33"><span class="brackets">RYN+21</span></dt>
<dd><p>Colorado J Reed, Xiangyu Yue, Ani Nrusimha, Sayna Ebrahimi, Vivek Vijaykumar, Richard Mao, Bo Li, Shanghang Zhang, Devin Guillory, Sean Metzger, Kurt Keutzer, and Trevor Darrell. Self-Supervised Pretraining Improves Self-Supervised Pretraining. <em>arXiv</em>, 2021. <a class="reference external" href="https://arxiv.org/abs/2103.12718">arXiv:2103.12718</a>.</p>
</dd>
<dt class="label" id="id26"><span class="brackets"><a class="fn-backref" href="#id5">ZCDLP17</a></span></dt>
<dd><p>Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond Empirical Risk Minimization. <em>arXiv</em>, 2017. <a class="reference external" href="https://arxiv.org/abs/1710.09412">arXiv:1710.09412</a>.</p>
</dd>
</dl>
</p>
</section>
</section>

<div class="section">
     
<div class="section">
  <span style="float: left">
    
  </span>
  <span>&nbsp;</span>
  <span style="float: right">
     Next: 
    <a href="../../Literature/darrell_selfsupervise/">
      Trevor Darrell’s Self Supervision Papers 
    </a>
    
  </span>
</div>
  
  <div class="section">
    <h2>Comments</h2>
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname = "taiga_projdoc";
      var disqus_identifier = "/Research/samplesgood/";
      var disqus_title = "Are More Training Samples Always Good?";
      var disqus_url = "https://cellistigs.github.io/Research/samplesgood";

      (function () {
        var dsq = document.createElement("script");
        dsq.type = "text/javascript";
        dsq.async = true;
        dsq.src = "//" + disqus_shortname + ".disqus.com/embed.js";
        (
          document.getElementsByTagName("head")[0] ||
          document.getElementsByTagName("body")[0]
        ).appendChild(dsq);
      })();
    </script>
    <noscript
      >Please enable JavaScript to view the
      <a href="https://disqus.com/?ref_noscript"
        >comments powered by Disqus.</a
      ></noscript
    >
    <a href="https://disqus.com" class="dsq-brlink"
      >comments powered by <span class="logo-disqus">Disqus</span></a
    >
  </div>
  
</div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../">taiga_projectdoc</a></h1>








  
<h2>
   
  10 August 2020 
</h2>

<ul>
   
<li id="author">
  <span
    >Author:</span
  >
   
  <a href="../../blog/author/me/">me</a>  
</li>
 
<li id="location">
  <span
    >Location:</span
  >
   
  <a href="../../blog/location/nyc/">NYC</a>  
</li>
 
<li id="language">
  <span
    >Language:</span
  >
   
  <a href="../../blog/language/en/">en</a>  
</li>
 
<li id="category">
  <span
    >Category:</span
  >
   
  <a href="../../blog/category/res/">Res</a>  
</li>
 
<li id="tags">
  <span
    >Tags: </span
  >
   
  <a href="../../blog/tag/data/">data</a>   
  <a href="../../blog/tag/ml/">ml</a>  
</li>
 
<li id="comments">
  <script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = "taiga_projdoc"; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
      var s = document.createElement("script");
      s.async = true;
      s.type = "text/javascript";
      s.src = "//" + disqus_shortname + ".disqus.com/count.js";
      (
        document.getElementsByTagName("HEAD")[0] ||
        document.getElementsByTagName("BODY")[0]
      ).appendChild(s);
    })();
  </script>
  
  <a
    href="#disqus_thread"
    data-disqus-identifier="/Research/samplesgood/"
  >
    Comments</a
  >
</li>

</ul>
<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../about/">About Taiga Abe</a></li>
</ul>


<h3>
  <a href="../../blog/">Recent Posts</a>
</h3>
<ul>
   
  <li>
    <a href="../../Literature/darrell_selfsupervise/"
      >10 August - Trevor Darrell’s Self Supervision Papers</a
    >
  </li>
  
</ul>

<h3><a href="../../blog/tag/">Tags</a></h3>
<style type="text/css">
  ul.ablog-cloud {
    list-style: none;
    overflow: auto;
  }
  ul.ablog-cloud li {
    float: left;
    height: 20pt;
    line-height: 18pt;
    margin-right: 5px;
  }
  ul.ablog-cloud a {
    text-decoration: none;
    vertical-align: middle;
  }
  li.ablog-cloud-1 {
    font-size: 80%;
  }
  li.ablog-cloud-2 {
    font-size: 95%;
  }
  li.ablog-cloud-3 {
    font-size: 110%;
  }
  li.ablog-cloud-4 {
    font-size: 125%;
  }
  li.ablog-cloud-5 {
    font-size: 140%;
  }
</style>
<ul class="ablog-cloud">
   
  <li class="ablog-cloud ablog-cloud-3">
    <a href="../../blog/tag/data/">data</a>
  </li>
    
  <li class="ablog-cloud ablog-cloud-3">
    <a href="../../blog/tag/data-augmentation/">data augmentation</a>
  </li>
    
  <li class="ablog-cloud ablog-cloud-3">
    <a href="../../blog/tag/ml/">ml</a>
  </li>
    
  <li class="ablog-cloud ablog-cloud-3">
    <a href="../../blog/tag/self-supervised-learning/">self-supervised learning</a>
  </li>
   
</ul>

<h3>
  <a href="../../blog/category/">Categories</a>
</h3>
<ul>
   
  <li>
    <a href="../../blog/category/lit/">Lit (1)</a>
  </li>
    
  <li>
    <a href="../../blog/category/res/">Res (1)</a>
  </li>
   
</ul>

<h3>
  <a href="../../blog/archive/">Archives</a>
</h3>
<ul>
   
  <li>
    <a href="../../blog/2020/">2020 (2)</a>
  </li>
   
</ul>

<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search/" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Taiga Abe.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/Research/samplesgood.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>