<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://cellistigs.github.io</id>
  <title>taiga_projectdoc Blog</title>
  <updated>2021-08-11T16:10:51.448913+00:00</updated>
  <link href="https://cellistigs.github.io"/>
  <link href="https://cellistigs.github.io/blog/atom.xml" rel="self"/>
  <generator uri="https://ablog.readthedocs.org/" version="0.10.19">ABlog</generator>
  <entry>
    <id>https://cellistigs.github.io/Research/samplesgood/</id>
    <title>Are More Training Samples Always Good?</title>
    <updated>2020-08-10T00:00:00-04:00</updated>
    <author>
      <name>me</name>
    </author>
    <content type="html">&lt;p&gt;Common knowledge in the machine learning community is that adding more data is always a good idea, and low data methods like n-shot learning (for small n) normally require the introduction of strong inductive biases in order to achieve good performance. However, if we think about the eventual goal of our networks as generalization, does adding more training data really always help? Some recent studies would suggest that the answer is no.&lt;/p&gt;
</content>
    <link href="https://cellistigs.github.io/Research/samplesgood/" rel="alternate"/>
    <summary>Common knowledge in the machine learning community is that adding more data is always a good idea, and low data methods like n-shot learning (for small n) normally require the introduction of strong inductive biases in order to achieve good performance. However, if we think about the eventual goal of our networks as generalization, does adding more training data really always help? Some recent studies would suggest that the answer is no.</summary>
    <category term="data" label="data"/>
    <category term="ml" label="ml"/>
    <published>2020-08-10T00:00:00-04:00</published>
  </entry>
</feed>
