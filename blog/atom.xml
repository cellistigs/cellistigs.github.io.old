<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://cellistigs.github.io</id>
  <title>taiga_projectdoc Blog</title>
  <updated>2021-08-25T21:43:16.174522+00:00</updated>
  <link href="https://cellistigs.github.io"/>
  <link href="https://cellistigs.github.io/blog/atom.xml" rel="self"/>
  <generator uri="https://ablog.readthedocs.org/" version="0.10.19">ABlog</generator>
  <entry>
    <id>https://cellistigs.github.io/Research/samplesgood/</id>
    <title>Are More Training Samples Always Good?</title>
    <updated>2020-08-10T00:00:00-04:00</updated>
    <author>
      <name>me</name>
    </author>
    <content type="html">&lt;p&gt;Common knowledge in the machine learning community is that adding more data is always a good idea, and low data methods like n-shot learning (for small n) normally require the introduction of strong inductive biases in order to achieve good performance. However, if we think about the eventual goal of our networks as generalization, does adding more training data really always help? Some recent studies would suggest that the answer is no.&lt;/p&gt;
</content>
    <link href="https://cellistigs.github.io/Research/samplesgood/" rel="alternate"/>
    <summary>Common knowledge in the machine learning community is that adding more data is always a good idea, and low data methods like n-shot learning (for small n) normally require the introduction of strong inductive biases in order to achieve good performance. However, if we think about the eventual goal of our networks as generalization, does adding more training data really always help? Some recent studies would suggest that the answer is no.</summary>
    <category term="data" label="data"/>
    <category term="ml" label="ml"/>
    <published>2020-08-10T00:00:00-04:00</published>
  </entry>
  <entry>
    <id>https://cellistigs.github.io/Literature/piketty_capital/</id>
    <title>Capital in the 21st Century: Notes</title>
    <updated>2020-08-12T00:00:00-04:00</updated>
    <author>
      <name>me</name>
    </author>
    <content type="html">&lt;p&gt;Some notes on Thomas Piketty’s book &lt;em&gt;Capital in the 21st Century&lt;/em&gt;. I don’t have an economics background, so lots of the stuff will be me figuring out basic economics concepts from Piketty’s analysis of them.&lt;/p&gt;
</content>
    <link href="https://cellistigs.github.io/Literature/piketty_capital/" rel="alternate"/>
    <summary>Some notes on Thomas Piketty’s book Capital in the 21st Century. I don’t have an economics background, so lots of the stuff will be me figuring out basic economics concepts from Piketty’s analysis of them.</summary>
    <category term="economics" label="economics"/>
    <published>2020-08-12T00:00:00-04:00</published>
  </entry>
  <entry>
    <id>https://cellistigs.github.io/Documentation/github_actions/</id>
    <title>Testing Github Actions Locally with Act</title>
    <updated>2021-08-17T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;One great thing I discovered in the course of working on the NeuroCAAS project is Continuous Integration/Continuous Deployment. Although these topics deserve their own posts, in brief I think about them as ways to automate all the stuff that has to happen in between you writing your source code, and someone else having a functional tool in their hands. This could mean &lt;em&gt;testing&lt;/em&gt; your source code, to make sure that it doesn’t break everything that’s already there. It could also mean &lt;em&gt;building&lt;/em&gt; your project, if you need to compile something, or &lt;em&gt;installing&lt;/em&gt; necessary dependencies. If you have a web service, it might also mean &lt;em&gt;deploying&lt;/em&gt; your tool so that others can access it through some pre-existing front end. Continuous * recognizes that all of these things have to happen, that they take time, and that failing to do them regularly can lead to long-lasting mistakes in your source code that you might not find until you’ve build a gigantic edifice around them already. This is especially true when we start thinking about the dependencies that our project might have on our native operating system, installed software, file system, etc.&lt;/p&gt;
</content>
    <link href="https://cellistigs.github.io/Documentation/github_actions/" rel="alternate"/>
    <summary>One great thing I discovered in the course of working on the NeuroCAAS project is Continuous Integration/Continuous Deployment. Although these topics deserve their own posts, in brief I think about them as ways to automate all the stuff that has to happen in between you writing your source code, and someone else having a functional tool in their hands. This could mean testing your source code, to make sure that it doesn’t break everything that’s already there. It could also mean building your project, if you need to compile something, or installing necessary dependencies. If you have a web service, it might also mean deploying your tool so that others can access it through some pre-existing front end. Continuous * recognizes that all of these things have to happen, that they take time, and that failing to do them regularly can lead to long-lasting mistakes in your source code that you might not find until you’ve build a gigantic edifice around them already. This is especially true when we start thinking about the dependencies that our project might have on our native operating system, installed software, file system, etc.</summary>
    <category term="software" label="software"/>
    <category term="testing" label="testing"/>
    <category term="continuousintegration" label="continuous integration"/>
    <category term="continuousdeployment" label="continuous deployment"/>
    <published>2021-08-17T00:00:00-04:00</published>
  </entry>
  <entry>
    <id>https://cellistigs.github.io/Documentation/aws_volumes_snapshots/</id>
    <title>AWS EC2 Storage (Volumes, AMIs and Snapshots)</title>
    <updated>2021-08-25T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;Title underline too short.&lt;/p&gt;
</content>
    <link href="https://cellistigs.github.io/Documentation/aws_volumes_snapshots/" rel="alternate"/>
    <summary>Title underline too short.</summary>
    <category term="software" label="software"/>
    <category term="AWS" label="AWS"/>
    <category term="storage" label="storage"/>
    <published>2021-08-25T00:00:00-04:00</published>
  </entry>
  <entry>
    <id>https://cellistigs.github.io/Research/resnet_50/</id>
    <title>ResNet-50</title>
    <updated>2021-08-25T00:00:00-04:00</updated>
    <author>
      <name>me</name>
    </author>
    <content type="html">&lt;p&gt;In the last few weeks I’ve gotten back into working directly with deep learning models, and I’ve been running into a lot of ResNet-50. It’s pretty interesting to me that this model developed in 2015 is still used as a standard benchmark in broad selection of different subfields of deep learning.&lt;/p&gt;
</content>
    <link href="https://cellistigs.github.io/Research/resnet_50/" rel="alternate"/>
    <summary>In the last few weeks I’ve gotten back into working directly with deep learning models, and I’ve been running into a lot of ResNet-50. It’s pretty interesting to me that this model developed in 2015 is still used as a standard benchmark in broad selection of different subfields of deep learning.</summary>
    <category term="ml" label="ml"/>
    <published>2021-08-25T00:00:00-04:00</published>
  </entry>
</feed>
