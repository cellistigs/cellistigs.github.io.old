<html>
<head>
    <link rel="Stylesheet" type="text/css" href="style.css" />
    <title>Chaining Analyses</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
</head>
<body> 
    <a href="index.html">Index</a> |
    <a href="diary/diary.html">Diary</a>
    <hr>
    <div class="content">
    
<div id="Chaining Analyses"><h1 id="Chaining Analyses" class="header"><a href="#Chaining Analyses">Chaining Analyses</a></h1></div>
<p>
One powerful extension of NeuroCAAS is the ability to chain together different analysis steps to achieve a larger goal. We want to accomplish this goal while maintaining the manageability and reproducibility of each individual analysis step. 
</p>

<p>
The solution we have come up with is to trigger AWS Lambda functions upon the completion of other analysis jobs. In each analysis blueprint, we can specify a Lambda Function that should be called upon analysis completion (Lambda.PostCodeUri, Lambda.PostHandler), giving us arbitrary flexibility to extend workflow between different analysis instances. We can chose to trigger this Lambda function on the upload of an arbitrarily named file to the results folder, <code>job__{analysis_name}_/process_results</code> (by default, this is the "end.txt" file generated by the logging function). 
</p>

<p>
By default, for multi-step analyses, there are two expected architectures to consider.
The first is multi-step, multi-blueprint analyses. In this case, the Postprocessing Lambda will point to a different blueprint, and enact some functions there. The analysis will have separate data, storage, and logs. This is a good choice if linking analyses that are loosely coupled, and one would want different infrastructure at each step. 
The second expected architecture is a multi-step, single blueprint analysis. In this case, we will trigger the same AMI off the end of the first round of processing, but do something different than we did before. This is a good choice if there are many intermediary outputs that you'd want to reuse over the course of analysis, and your infrastructure requirements do not change too much. You can indicate which of these you would like to consider in the "DestinationPipelineName" field.  
</p>

<p>
Both of these analyses can be implemented using the module ncap_iac/protocols/postprocess.py. The PostProcess object class here takes as input a source bucket and endfile key, a target bucket, and a step name to identify with the step you're about to start working on. If the source and target bucket are the same, you can just write code to create the correct config file, and reference the data correctly and you should be fine to submit. If the source and target bucket are different, you will have to copy over data into the target bucket, as there is no way to trigger jobs based on data located in other buckets. 
</p>

    </div>
    <p><small>Page created on 2021-08-01</small></p>
</body>
</html>
